%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% This is the template for submission to HPCA 2018
% The cls file is a modified from  'sig-alternate.cls'
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass{sig-alternate}
\setlength{\paperheight}{11in}
\setlength{\paperwidth}{8.5in}

\newcommand{\ignore}[1]{}
\usepackage[pass]{geometry}
\usepackage{fancyhdr}
\usepackage[normalem]{ulem}
\usepackage[hyphens]{url}
\usepackage{hyperref}
\usepackage{amsmath}

%%%%%%%%%%%---SETME-----%%%%%%%%%%%%%
\newcommand{\hpcasubmissionnumber}{XXX}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\fancypagestyle{firstpage}{
  \fancyhf{}
\setlength{\headheight}{50pt}
\renewcommand{\headrulewidth}{0pt}
  \fancyhead[C]{\normalsize{HPCA 2019 Submission
      \textbf{\#\hpcasubmissionnumber} -- Confidential Draft -- Do NOT Distribute!!}}
  \pagenumbering{arabic}
}

%%%%%%%%%%%---SETME-----%%%%%%%%%%%%%
\title{Resource and Runtime Efficiency for Multi Algorithmic Fibonacci Algorithm}
\author{}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}
\maketitle
\thispagestyle{firstpage}
\pagestyle{plain}



%%%%%% -- PAPER CONTENT STARTS-- %%%%%%%%

\begin{abstract}

This investigation explores and compares runtime efficiency and resource consumption 
for both recursive and dynamic programming across several different programming languages that abstract widely differing architectural elements. Utilizing the Fibonacci algorithm, we show that for the algorithms investigated, compiled languages demonstrated measurably better runtime efficiency than the in-ter-preted languages studied. An example is the compiled Go language. Compared to Python, which is in-ter-preted, Go was on av-erage, 1267.44 percent faaster in execution for the recursive algorithm, but 1395.18 percent slower for the dynamic algorithm. At the same time, the results indicated that the compiled languages required more computational resource in pursuit of this faster execution. Looking again at Go compared to Python for the recursive algorithm, Go utilized, on average, 100.02 percent of available processor resource versus 97.67 percent for Python. As for the dynamic algorithm, Go utilized, on average, 101.92 percent of the available processor resource, and Python utilized, on average, 94.92 percent. These results are interesting, taking into account lower machine instruction counts for the compiled languages. A full comparison of all studied languages is presented, the potential factors behind the results analyzed and the possible ramifications for actual use discussed.

\end{abstract}

\section{Introduction}
Algorithmic efficiency has become hugely important due to the need to analyze massive data sets generated by cloud computing and the Internet of Things. While making algorithms more succinct and comprehensive, recursion can also be highly inefficient when applied to these data sets. As an alternative to recursion in many applications, dynamic programming techniques can significantly improve runtime efficiency. Although runtime efficiency has been widely studied for specific problem applications, less attention has been given to the relationship of language and underlying architecture to a broader measure of efficiency that includes both runtime and resource consumption.

Different programming languages are utilized for different purposes, which leads to the question of when to use one language over another? Compiled languages utilize a compiler to take the whole program as input and compile it only once. They generally execute faster than interpreted languages and take up more memory to create the object code as output [1]. Interpreted languages utilize an interpreter, which reads in a single line of code at a time. Because the syntax tree is processed directly to evaluate or execute statements, some of the code may be processed over and over again, resulting in slower execution for interpreted languages [1]. Some common examples of compiled languages include C and Go. Common examples of interpreted languages include Python and Perl. 

This paper presents comparisons between several different programming languages, to include interpreted and compiled, and analyzes their performance efficiencies. As a way to make Python more comparable to the compiled language of C, ways of code optimization were explored. As a result, a version of inline C and a Python implementation with a decorator function to store the results needed later for computation were tested.

\section{Background}

In this section, there will be a list of several key concepts and formulas relevant to this research. While this work focuses mainly on C and Python, the generalizations mentioned can be applied to the general concept of compiled languages as opposed to interpreted languages. 

\subsection{Programming Languages}	

We utilized four languages in our study:

\begin{enumerate}
\item {\em}C is a compiled language created in 1972 at Bell Labs for UNIX system implementation [1]. C is the basis of the programming languages of Java and C++. It is often chosen when speed is a priority for inputs consisting of large data sets [2]. Although it isn't the most simple language to develop with, it is a major player in High Performance Computing (HPC) because of its efficiency in performance [3].
\item {\em}Go is a relatively new programming language created by Google. Go is useful for systems programming and scalable network servers. It is a close second behind C when it comes to speed, and is a relatively simple language to develop with [3]. It is based upon C's implementation; however, it was developed with a focus on a simpler design for the programmer [3]. 
\item {\em}Python is an interpreted language created in 1991 [4]. It has recently made a presence in the HPC world through its packages, such as NumPy, SciPy, and scikit-learn; with these pack-ages, Python applications are optimized to take full 
ad-vantage of the present architecture [4]. With such a presence in HPC, it is clear that Python is here to stay.
\item  {\em}Perl is a common scripting language that utilizes an interpreter. Like Python, it too, has a large variety of libraries to pull from. Perl is quicker and more efficient than Python in term of input/output operations [5]. In addition, Perl is a useful language due to it having a large number of parallel computing modules available.
\end{enumerate}


\subsection{Decorated Python}

As a way to make Python more comparable to the execution speed of C, a decorator function with a wrapper was implemented. Decorator functions are more commonly utilized for tracing, locking, or logging [6]. However, a decorator function combined with a Python wrapper can also be created as a way to make a program more dynamic by having it remember the results needed later for computation. This allowed its performance to be much quicker than that of C in terms of execution time.

\subsection{Inline C}

A method to optimize C was also explored utilizing the keyword ''inline'' before the function call. Inline is a useful tool with smaller functions that will be called multiple times in order to reduce function overhead. Utilizing the inline keyword reduces function call overhead by replacing the actual call with the contents of the function itself. Because of this replacement of the function call with the function contents, it is not very useful with multiple recursive calls as there will be a tradeoff in terms of instruction count and efficiency.

\section{Experimental Methodology}

In the following subsections, the different aspects of this study will be described.

\subsection{Environment}

In this subsection, the elements used to set up the environment will be described. We used an Intel NUC NUC5CPYH with Ubuntu 16.04 installed, a gcc version of 5.4.0, and python 2.7. We installed the other necessary languages for the experiment, so that the NUC had Perl, Python, C, and Go installed. We also installed perf, a resource monitoring utility, so that we could monitor the resource consumption of the different languages.    

\subsection{Execution}

The Fibonacci algorithm gathered from Rosetta Code was utilized as the testing algorithm [8]. Rosetta Code is a repository which contains different algorithms with many programming languages to choose from. A recursive and dynamic version of the Fibonacci algorithm was used from this site for the testing algorithm. Fibonacci values of 20, 30, 40, and 50 were used as function parameters for testing purposes.

\subsection{Analysis}

A profile monitoring resource was used to monitor the resource consumption of the algorithms in order to collect data. Perf is a sample based Linux profiler which monitors Linux perf events [9]. It was utilized for every trial, testing task-clock, CPU-cycles, instruction count, the number of CPUs utilized, clock rate, instructions per second, elapsed time, page-faults, cache-misses, and percent of all cache references. The time command was also used to calculate the time sum consisting of user and system time. Speedup was calculated using the execution times between the different languages.

\section{Results}

The results for this section will be split up according to algorithm results, programming language results, and optimized programming language results. The optimized language results will include the inline C and decorated Python result explanations.

\subsection{Algorithm Results}

The overall runtime for the recursive algorithms were larger than the dynamic algorithm runtimes. The overall trend for the recursive algorithms included an increase in resource consumption for the Fibonacci numbers of 30 and 40, whereas the dynamic algorithm results trended mostly linearly. Because of the characteristics of recursive algorithms, it is expected for the runtimes to be slower as there is higher overhead from the call stack being used so heavily. Furthermore, programs are bounded by physical memory, so it is likely that Perl, Python, and Go reached their limit. Otherwise, as the compiler was setting up the activation records, it was trying to do something fancy with the algorithm; thus, causing issues for the runtimes. The dynamic algorithm did not have these issues as there is little overhead. The CPU-cycles may have been reported incorrectly as perf is sample based, and does not count every cycle. It is likely the programs ran too fast and perf didn’t catch all of the CPU-cycles. In addition, the task-clock and instruction count results can be explained by the higher overhead. For larger Fibonacci numbers, it became unfeasible to calculate the resources as the stack grew too large for the recursive algorithm. The dynamic algorithms were more linear as the resources needed to calculate the larger Fibonacci numbers became higher, because more resources are necessary to deal with larger input values.

\subsection{Language Results}
The runtimes of Python and Perl appear to be essentially infinite near the Fibonacci number of 30 with the result that no data could be collected for higher numbers. Perl showed a recursive runtime increase of 99.97 percent up to the Fibonacci number of 40. It’s interesting that Go was 99.97 percent slower than C, another compiled language, for the recursive algorithm. Also surprisingly, the dynamic algorithm showed a higher runtime for C and Go, versus Perl and Python rather than the inverse, which was expected. Being that interpreted languages are interpreted one line at a time, they generally execute slower. However, in this experiment, they executed faster for the dynamic algorithm. This inconsistency can be explained by the default optimization levels of each of the programs potentially being different, or issues with the scope of the Fibonacci numbers. In addition, Go had an unusually high resource consumption possibly due to perf not calculating the results properly as its results are sample based. Furthermore, all languages should have increased more for the dynamic algorithmic instruction count, as the overhead increased; however, this was not the case. The task-clock rates also should have increased more, but there was a maximum increase of 3.35 percent for the language of Perl.

\subsection{C Versus Python Results}
C consistently outperformed Python on task-clock time, CPU-cycles, instruction count, instructions per second, and elapsed time. When it came to speedup calculations, C greatly outperformed Python by at least a factor of 28 over Python for Fibonacci of 20 through Fibonacci of 40, with the number 5 chosen for the test intervals. Regarding the memory resources, Python had significantly more page faults than C. For both the recursive algorithm, and the dynamic algorithm, C consistently had around 41 page faults, whereas Python had at least 797 page faults for both its recursive and dynamic algorithm for all Fibonacci numbers evaluated. As for cache misses, Python also fared worse. Python showed exponential growth for its recursive algorithm cache misses, averaging 269,000 cache misses, whereas C averaged 10,500 for its cache misses, and also depicted exponential growth. Python also had more cache misses in the dynamic algorithm with an average of 270,000 cache misses, whereas C had an average of 7820 cache misses for the dynamic algorithm. In addition, the cache misses percent of all cache references decreased substantially for Python from 16\% to just above 0\% for Fibonacci values of 20 through 40; this trend was similar to C\'s performance, but from a starting point of 25\% down to 10\%. A cache miss is when the data requested for is not in the cache memory and requires the program to fetch the data from other cache levels, or main memory. These high percentages for the cache misses as a percent of all cache references can lead to significant delays in execution time to due the necessity of travelling up the memory heirarchy.

\subsection{Inline C Versus C Results}
C remained under 1 millisecond for all Fibonacci numbers tested, but inline C went all the way up to 3247.92 milliseconds. C utilized 858,889,235,221 CPU cycles for Fibonacci of 50, whereas inline C only went through 6,971,907,740 CPU cycles. Inline C utilized a lot more instructions than C did; being that the idea behind the keyword “inline” reduces the amount of function calls by replacing the function call with the contents of the function itself, inline C resulted in more instructions. Both forms of C had about the same number of CPUs utilized. Interestingly, inline C used 100\% less GHz for its clock rate compared to C for Fibonacci of 20. However, inline C and C grew to about the same clock rate after Fibonacci of 50. Initially, inline C had more than double the number of instructions per second than C; however, as the Fibonacci value increased for the function parameter input, the processor speed for both equated to about the same. Both inline C and C had the same average number of page-faults, cache misses, and percent of all cache refs being cache misses. The one small difference between the two is the change in their percent of all cache references being cache misses. According to the graphs, C increasing a bit, and then gradually decreasing, while inline C decreased drastically, increased a bit, and then decrease at a more gradual pace down to almost 0\%.

\subsection{Decorated Python Versus Python Results}
Python took longer for its task clock rates than that of Decorated Python with Fib of 40 being 91871.376 milliseconds for Python, and about 36 milliseconds for Decorated Python. Decorated Python used far fewer CPU cycles with its highest number of CPU cycles at 60976899.33 and Python at 132,298,225,415 cycles for Fibonacci of 40. Decorated Python also used far fewer instructions and had a smaller clock rate. Decorated Python executed 28,075,491.67 instructions. Python executed 251,140,228,546 instructions, which is 89.45\% more instructions for Python. In terms of memory access, Decorated Python was costlier because of its large amount of page faults. However, decorated Python significantly improved its cache misses from Python. For Fibonacci of 40, Python had 42,700,000 cache misses and Decorated Python had 274,928 cache misses. Decorated Python maintained about 270,000 cache misses for all Fibonacci values tested, yet Python increased exponentially up from 269,000 cache misses up to 427,000 cache misses. It is also interesting to note that Python had a continual decline down to almost 0\% for its cache misses as a percent of all cache references; Decorated Python remained constant at 15\% for Fibonacci values of 20 through 100. Python overall had a continual decline down to almost 0\% for its cache misses as a percent of the total cache references, whereas Decorated Python remained constant at 15\% for Fibonacci of 20 through Fibonacci of 100. Python took longer for its task clock rates than that of decorated Python with Fibonacci of 40 being 91,871.376 milliseconds for Python, and about 36 milliseconds for Decorated Python. Decorated Python used fewer CPU cycles with its highest at 60,976,899.33 CPU cycles, and Python\'s highest at 132,298,225,415 CPU cycles for Fibonacci of 40. With regard to memory access, Decorated Python was had a few more page faults than Python. Decorated Python had anywhere from 810 to 820 page faults. Python had at most 799 page faults.

\subsection{C Versus Decorated Python Results}
C was faster than Decorated Python with a speedup of almost 30 for Fibonacci of 20. Contrarily, the two switched places after Fibonacci of 30, with Decorated Python ending Fibonacci of 40 almost 90 times faster than C. Decorated Python had fewer CPU cycles at 60,976,899.33 cycles, with C at 858,889,235,221. Decorated Python outperformed C in regard to instruction count, having 28,075,491.67 instructions, and C at 6,169,788,902. Both used about 0.9 CPUs, but C used 2.156 GHz, and Decorated Python only used 1.666 GHz for Fibonacci of 40. That is a difference of 77.27\%. On the other hand, Decorated Python had at least 810 page faults for every Fibonacci value tested, whereas C had at most 41 page faults. Decorated Python was also costlier for cache misses. However, Cs cache misses as a percent of all cache references decreased from about 25\% to 10\% and Decorated Python maintained 15\% for every trial.

\begin{scriptsize}
\begin{table}[ht!]
  \centering
  \begin{tabular}{|l|l|}
    \hline
    \textbf{Fibonacci Number} & \textbf{Execution Time (sec)}\\
    \hline
    20 & 0.04\\
    \hline
    25 & 0.007333\\
	\hline
	30 & 0.01\\
	\hline
	35 & 0.296667\\
	\hline
	40 & 1.2967\\
	\hline
\end{tabular}
\caption{Execution time results for the recursive algorithm for C.}
\label{table:formatting}
\end{table}
\end{scriptsize}

\begin{scriptsize}
\begin{table}[ht!]
  \centering
  \begin{tabular}{|l|l|}
    \hline
    \textbf{Fibonacci Number} & \textbf{Execution Time (sec)}\\
    \hline
    20 & 0.0036\\
    \hline
    25 & 0.005333\\
	\hline
	30 & 0.032\\
	\hline
	35 & 0.29333\\
	\hline
	40 & 3.2386667\\
	\hline
\end{tabular}
\caption{Execution time results for the recursive algorithm for Inline C.}
\label{table:formatting}
\end{table}
\end{scriptsize}

\begin{scriptsize}
\begin{table}[ht!]
  \centering
  \begin{tabular}{|l|l|}
    \hline
    \textbf{Fibonacci Number} & \textbf{Execution Time (sec)}\\
    \hline
    20 & 0.0097\\
    \hline
    25 & 0.107333\\
	\hline
	30 & 0.36\\
	\hline
	35 & 8.330167\\
	\hline
	40 & 82.847\\
	\hline
\end{tabular}
\caption{Execution time results for the recursive algorithm for Python.}
\label{table:formatting}
\end{table}
\end{scriptsize}

\begin{scriptsize}
\begin{table}[ht!]
  \centering
  \begin{tabular}{|l|l|}
    \hline
    \textbf{Fibonacci Number} & \textbf{Execution Time (sec)}\\
    \hline
    20 & 0.03\\
    \hline
    25 & 0.031333\\
	\hline
	30 & 0.0316667\\
	\hline
	35 & 0.031333\\
	\hline
	40 & 0.032333\\
	\hline
\end{tabular}
\caption{Execution time results for the recursive algorithm for Decorated Python.}
\label{table:formatting}
\end{table}
\end{scriptsize}

In each of the tables above, the time exponentially increases as the Fibonacci value increments for the languages of C, Inline C, and Python. Table 3 depicts Python growing at the quickest speed, having a periodic growth rate of 1.18 seconds per Fibonacci Number increment. It's counterpart, Decorated Python, in Table 4 possesses the slowest exponential growth at 0.015 for its periodic growth rate. C in Table 1 is higher at 0.69 for its periodic growth rate, and Inline C shows a periodic growth rate of 1.36. It is interesting to see an interpretted language optimized with a decorated function beating out one of the quickest compiled languages.

\subsection{Summary Statements}

It was shown that the common thought of compiled languages beating out interpretted languages in terms of execution time is not always the case; optimizations can be made to make Python a valid competitor to the compiled language of C. Even Inline C could not compete with Decorated Python when it came down to execution time. At the same time, C outperformed Python in almost every other regard. Python was much less efficient with memory allocation, resulting in more page faults and cache misses for its results. This can be due to the differences in the way that Python and C structure their memory layouts. C utilizes a stack frame, whereas Python ustilizes Python objects and a private heap. Utilizing the stack is proven to be more efficient than using a heap, as a stack is trivial to allocate and deallocate memory. The heap takes longer to access due to its more complex bookkeeping of memory allocation and freeing of memory.

In addition, with many recursive calls, C would be the better choice with a faster execution time. However, inline C used far fewer CPU cycles, with hundreds of millions more instructions than C did. When it comes down to the cache, there is little difference between Inline C and C.

Having a decorated Python function takes a bit longer to execute, but it significantly improved the number of CPU cycles utilized, the number of instructions executed, and far fewer cache misses. Decorated Python does take a hit with more page faults and a constant of about 15\% of all cache references being cache misses. Decorated Python outperformed C in regard to execution time and speedup, CPU cycles, clock rate, and instruction count. However, Decorated Python was far costlier in terms of page faults, and cache misses.

\subsection{Conclusion}

Due to the massive data sets created by the Internet of things and cloud computing, algorithmic efficiency is of the utmost importance in today's computing environment. In this paper we have looked at several programming languages depicting widely differing architectures, to include compiled and interpreted languages. Their performance was analyzed using the Fibonacci algorithm with Fibonacci of 20 up to Fibonacci of 50 with 5 as the increment. 

\noindent\textbf{\sout{Author List.}} Reviewing will be double blind;
therefore, please do not include any author names on any submitted
documents except in the space provided on the submission form.  You must
also ensure that the metadata included in the PDF does not give away the
authors. If you are improving upon your prior work, refer to your prior
work in the third person and include a full citation for the work in the
bibliography.  For example, if you are building on {\em your own} prior
something like: "While the authors of
additionally does W, and is therefore much better."  Do NOT omit or
anonymize references for blind review.  There is one exception to this for
your own prior work that appeared in IEEE CAL, workshops without archived
proceedings, etc.\, as discussed later in this document.

\noindent\textbf{Figures and Tables.} Ensure that the figures and tables
are legible.  Please also ensure that you refer to your figures in the main
text.  Many reviewers print the papers in gray-scale. Therefore, if you use
colors for your figures, ensure that the different colors are highly
distinguishable in gray-scale.

\noindent\textbf{References.}  There is no length limit for references.
{\bf Each reference must explicitly list all authors of the paper.  Papers
not meeting this requirement will be rejected.} Authors of NSF proposals
should be familiar with this requirement. Knowing all authors of related
work will help find the best reviewers. Since there is no length limit
for the number of pages used for references, there is no need to save space
here.

\section{Paper Submission Instructions}

\subsection{Guidelines for Determining Authorship}


IEEE guidelines dictate that authorship should be based on a {\bf
  substantial intellectual contribution}. It is assumed that all
authors have had a significant role in the creation of an article that
bears their names. In particular, the authorship credit must be
reserved only for individuals who have met each of the following
conditions:

\begin{enumerate}

\item Made a significant intellectual contribution to the theoretical
  development, system or experimental design, prototype development,
  and/or the analysis and interpretation of data associated with the
  work contained in the article;

\item Contributed to drafting the article or reviewing and/or revising
  it for intellectual content; and

\item Approved the final version of the article as accepted for
  publication, including references.

\end{enumerate}

A detailed description of the IEEE authorship guidelines and
responsibilities is available
\href{https://www.ieee.org/publications_standards/publications/rights/Section821.html}{here}.
Per these guidelines, it is not acceptable to award {\em honorary }
authorship or {\em gift} authorship. Please keep these guidelines in
mind while determining the author list of your paper.


\subsection{Declaring Authors}

Declare all the authors of the paper upfront. Addition/removal of authors
once the paper is accepted will have to be approved by the program chair,
since it potentially undermines the goal of eliminating conflicts for
reviewer assignment.


\subsection{Areas and Topics}

Authors should indicate these areas on the submission form as
well as specific topics covered by the paper for optimal reviewer match. If
you are unsure whether your paper falls within the scope of HPCA, please
check with the program chair -- HPCA is a broad, multidisciplinary
conference and encourages new topics.

\subsection{Declaring Conflicts of Interest}

Authors must register all their conflicts on the paper submission site.
Conflicts are needed to ensure appropriate assignment of reviewers.
If a paper is found to have an undeclared conflict that causes
a problem OR if a paper is found to declare false conflicts in order to
abuse or ``game'' the review system, the paper may be rejected.

Pease declare a conflict of interest (COI) with the following people for any author of your paper:

\begin{enumerate}
\item Your Ph.D. advisor(s), post-doctoral advisor(s), Ph.D. students,
      and post-doctoral advisees, forever.
\item Family relations by blood or marriage, or their equivalent,
      forever (if they might be potential reviewers).
\item People with whom you have collaborated in the last FIVE years, including
\begin{itemize}
\item co-authors of accepted/rejected/pending papers.
      whom you fund.
\end{itemize}
\item People (including students) who shared your primary institution(s) in the
last FIVE years.
\item Other relationships, such as close personal friendship, that you think might tend
to affect your judgment or be seen as doing so by a reasonable person familiar
with the relationship.
\end{enumerate}

``Service'' collaborations such as co-authoring a report for a professional
organization, serving on a program committee, or co-presenting
tutorials, do not themselves create a conflict of interest.
Co-authoring a paper that is a compendium of various projects with
no true collaboration among the projects does not constitute a
conflict among the authors of the different projects.

On the other hand, there may be others not covered by the above with
whom you believe a COI exists, for example, an ongoing collaboration
which has not yet resulted in the creation of a paper or proposal.
Please report such COIs; however, you may be asked to justify them.
Please be reasonable. For example, you cannot declare a COI with a
reviewer just because that reviewer works on topics similar to or
related to those in your paper.  The PC Chair may contact co-authors
to explain a COI whose origin is unclear.

We hope to draw most reviewers from the PC and the ERC, but others from the
community may also write reviews.  Please declare all your conflicts (not
just restricted to the PC and ERC).  When in doubt, contact the program
chair.

\subsection{Concurrent Submissions and Workshops}

By submitting a manuscript to HPCA'18, the authors guarantee that the
manuscript has not been previously published or accepted for publication in
a substantially similar form in any conference, journal, or the archived
proceedings of a workshop (e.g., in the ACM digital library) -- see
exceptions below. The authors also guarantee that no paper that contains
significant overlap with the contributions of the submitted paper will be
under review for any other conference or journal or an archived proceedings
of a workshop during the HPCA'18 review period. Violation of any of these
conditions will lead to rejection.

The only exceptions to the above rules are for the authors' own papers
in (1) workshops without archived proceedings such as in the ACM
digital library (or where the authors chose not to have their paper
appear in the archived proceedings), or (2) venues such as IEEE CAL
where there is an explicit policy that such publication does not
preclude longer conference submissions.  In all such cases, the
submitted manuscript may ignore the above work to preserve author
anonymity. This information must, however, be provided on the
submission form -- the PC chair will make this information available
to reviewers if it becomes necessary to ensure a fair review.  As
always, if you are in doubt, it is best to contact the program chair.


Finally, we also note that the IEEE Plagiarism Guidelines \href{http://www.ieee.org/publications\_standards/publications/rights/plagiarism.html}{IEEE Plagiarism Guidelines} covers a range of ethical issues concerning the misrepresentation of other works or
one's own work.

\section{Acknowledgements}
This document is derived from previous conferences, in particular HPCA 2017.  We thank Daniel A. Jimenez,  Elvira Teran for their inputs.


%%%%%%% -- PAPER CONTENT ENDS -- %%%%%%%%


%%%%%%%%% -- BIB STYLE AND FILE -- %%%%%%%%
\bibliographystyle{ieeetr}
\bibliography{ref}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\end{document}
